{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quality testing:** The process of checking data for defects in order to prevent system failures\n",
    "\n",
    "\n",
    "ETL quality testing involves 7 validation elements:\n",
    "\n",
    "1. **Completeness:** Confirming that the data contains all desired components or measures.\n",
    "2. **Consistency:** Confirming that data is compatible and in agreement across all systems.\n",
    "3. **Conformity:** Confirming that the data fits the required destination format.\n",
    "4. **Accuracy:** Confirming that the data conforms to the actual entity being measured or described.\n",
    "5. **Redundancy:** Moving, transforming, or storing more than the necessary data.\n",
    "6. **Integrity:** Confirming the data is accurate, complete, consistent, and trustworthy throughout its life cycle.\n",
    "7. **Timeliness:** Confirming that data is current.\n",
    "\n",
    "\n",
    "**Data mapping:** \n",
    "\n",
    "The process of matching fields from one data source to another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Seven elements of quality testing**\n",
    "\n",
    "\n",
    "In this part of the course, you have been learning about the importance of quality testing in your ETL system. This is the process of checking data for defects in order to prevent system failures. Ideally, your pipeline should have checkpoints built-in that identify any defects before they arrive in the target database system. These checkpoints ensure that the data coming in is already clean and useful! In this reading, you will be given a checklist for what your ETL quality testing should be taking into account. \n",
    "\n",
    "\n",
    "Checking for data quality involves ensuring the data is trustworthy before reaching its destination. When considering what checks you need to ensure the quality of your data as it moves through the pipeline, there are seven elements you should consider:\n",
    "\n",
    "- **Completeness:** Does the data contain all of the desired components or measures?\n",
    "\n",
    "- **Consistency:** Is the data compatible and in agreement across all systems?\n",
    "\n",
    "- **Conformity:** Does the data fit the required destination format?\n",
    "\n",
    "- **Accuracy:** Does the data conform to the actual entity being measured or described?\n",
    "\n",
    "- **Redundancy:** Is only the necessary data being moved, transformed, and stored for use?\n",
    "\n",
    "- **Timeliness:** Is the data current?\n",
    "\n",
    "- **Integrity:** Is the data accurate, complete, consistent, and trustworthy? (Integrity is influenced by the previously mentioned qualities.) Quantitative validations, including checking for duplicates, the number of records, and the amounts listed, help ensure data’s integrity.\n",
    "\n",
    "## **Common issues**\n",
    "\n",
    "There are also some common issues you can protect against within your system to ensure the incoming data doesn’t cause errors or other large-scale problems in your database system:\n",
    "\n",
    "- **Check data mapping:** Does the data from the source match the data in the target database?\n",
    "\n",
    "- **Check for inconsistencies:** Are there inconsistencies between the source system and the target system?\n",
    "\n",
    "- **Check for inaccurate data:** Is the data correct and does it reflect the actual entity being measured?\n",
    "\n",
    "- **Check for duplicate data:** Does this data already exist within the target system?\n",
    "\n",
    "To address these issues and ensure your data meets all seven elements of quality testing, you can build intermediate steps into your pipeline that check the loaded data against known parameters. For example, to ensure the timeliness of the data, you can add a checkpoint that determines if that data matches the current date; if the incoming data fails this check, there’s an issue upstream that needs to be flagged. Considering these checks in your design process will ensure your pipeline delivers quality data and needs less maintenance over time.\n",
    "\n",
    "**Key takeaways**\n",
    "\n",
    "One of the great things about BI is that it gives us the tools to automate certain processes that help save time and resources during data analysis– building quality checks into your ETL pipeline system is one of the ways you can do this! Making sure you are already considering the completeness, consistency, conformity, accuracy, redundancy, integrity, and timeliness of the data as it moves from one system to another means you and your team don’t have to check the data manually later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../images/d_quality_integrity_header.png\"></img>\n",
    "\n",
    "<img src=\"../../images/quality_integrity.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
