{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get started with data modeling, schemas, and databases\n",
    "\n",
    "\n",
    "<img src=\"../../images/path_to_insights.png\"></img>\n",
    "\n",
    "<img src=\"../../images/path_2_summary.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 1:\n",
    "\n",
    "Data models and pipelines\n",
    "You will start this course by exploring data modeling foundations, as well as common schemas and key database elements. You will also consider how business needs determine the kinds of database systems that a BI professional might implement. You will then shift to learning about pipelines and ETL processes, which are the tools that move data throughout the system and make sure it’s accessible and useful. Along the way, you will add many more important tools to your BI toolbox.\n",
    "\n",
    "## Week 2:\n",
    "\n",
    "Dynamic database design\n",
    "In this part of the course, you will learn more about database systems, including data marts, data lakes, data warehouses, and ETL processes. You will also investigate the five factors of database performance: workload, throughput, resources, optimization, and contention. Finally, you will begin thinking about how to design efficient queries that get the most from your system. \n",
    "\n",
    "## Week 3:\n",
    "\n",
    "Optimize ETL processes\n",
    "In this section of the course, you will learn about ETL quality testing, data schema validation, verifying business rules, and general performance testing. You will also explore data integrity and learn how built-in quality checks help you discover data defects. Finally, you will learn how to verify business rules and conduct general performance testing to make sure pipelines fulfill the intended business need. \n",
    "\n",
    "## Week 4:\n",
    "\n",
    "Course 2 end-of-course project\n",
    "In the second end-of-course project, you will create a pipeline process to deliver necessary data to a target table. Then, you will use that target table to develop reports based on project needs. After you create the pipeline, you will also ensure that it is performing correctly and that there are built-in defenses against data quality issues. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you will learn:\n",
    "\n",
    "- Data’s function within an organization\n",
    "- Why structured data is essential to business intelligence\n",
    "- How data models can be used to organize database systems\n",
    "- The distinctions between creating and querying a data model\n",
    "- The role of data warehouses, data marts, and data lakes in business intelligence\n",
    "- Strategies for creating and maintaining processes that meet organizational and stakeholder needs\n",
    "\n",
    "Skill sets you will build:\n",
    "\n",
    "- Applying data modeling to organize data elements and how they relate to one another\n",
    "- Retrieving data from a data source using pipelines, such as ETL\n",
    "- Transforming data into a usable format to answer specific business questions\n",
    "- Designing data pipelines that automate business intelligence processes\n",
    "- Using data modeling to design data storage systems, including data warehouses, data marts, and data lakes\n",
    "- Using actual data to create a business intelligence portfolio project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data lake:**\n",
    "A database system that stores large amounts of raw data in its original format unitl it´s needed.\n",
    "\n",
    "**Online Transaction Processing (OLTP) database:**\n",
    "A database that has been optimized for data processing instead of analysis.\n",
    "\n",
    "**Data mart:**\n",
    "A subject-oriented database that can be a subset of a larger data warehouse\n",
    "\n",
    "**Online Analytical Processing (OLAP) system:**\n",
    "A tool that has been optimized for analysis in addition to processing and can analyze data from multiple databases.\n",
    "\n",
    "\n",
    "Two types of data:\n",
    "\n",
    "1. **Unstructured data:** Data that is not organized in any easily identifiable manner.\n",
    "2. **Structured data:** Data that has been organized in a certain format such as rows and columns.\n",
    "\n",
    "**Data model:** A tool for organizing data elements and how they relate to one another.\n",
    "\n",
    "**Design pattern:** A solution that uses relevant measures and facts to create a model in support of business needs.\n",
    "\n",
    "**Schema:** A way of describing how something, such as data, is organized.\n",
    "\n",
    "**Common schemas:**\n",
    "- Relational models\n",
    "- Star schemas\n",
    "- Snowflake schemas\n",
    "- NoSQL schemas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Relational database:**\n",
    "\n",
    "A database that contains a series of tables that can be connected to form relationships.\n",
    "\n",
    "\n",
    "**Primary key**\n",
    "An identifier in a databse that references a column or group of columns in which each row uniquely identifies each record in the table.\n",
    "\n",
    "**Dimensional model**\n",
    "A type of relational model that has been optimized to quickly retrieve data from a data warehouse.\n",
    "\n",
    "**Fact**\n",
    "A measurement or metric.\n",
    "\n",
    "**Dimension (data modeling)**\n",
    "A piece of information that provides more detail and context regarding a fact.\n",
    "\n",
    "**Attribute**\n",
    "A characteristic or quality used to describe a dimension.\n",
    "\n",
    "**Fact table**\n",
    "A table that contains measurements or metrics related to a particular event.\n",
    "\n",
    "**Dimension table**\n",
    "The table where the attributes of the dimensions of a fact are stored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Types of schemas**\n",
    "\n",
    "**Star schema**\n",
    "A schema consisting of one fact table that references any number of dimension tables.\n",
    "\n",
    "\n",
    "<img src=\"../../images/star.png\"></img>\n",
    "\n",
    "**Snowflake schema**\n",
    "An extension of a star schema with additional dimensions and, often, subdimensions.\n",
    "\n",
    "<img src=\"../../images/snowflake.png\"></img>\n",
    "\n",
    "**Flat schema**\n",
    "Extremely simple database systems with a single table in which each record is represented by a single row of data. Flat models are not relational; they can´t capture relationships between tables or data items.\n",
    "\n",
    "<img src=\"../../images/flat.png\"></img>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Semi-structured schemas**\n",
    "\n",
    "In addition to traditional, relational schemas, there are also semi-structured database schemas which have much more flexible rules, but still maintain some organization. Because these databases have less rigid organizational rules, they are extremely flexible and are designed to quickly access data.\n",
    "\n",
    "There are four common semi-structured schemas:\n",
    "\n",
    "**Document schemas** store data as documents, similar to JSON files. These documents store pairs of fields and values of different data types.\n",
    "\n",
    "**Key-value schemas** pair a string with some relationship to the data, like a filename or a URL, which is then used as a key. This key is connected to the data, which is stored in a single collection. Users directly request data by using the key to retrieve it.\n",
    "\n",
    "**Wide-column schemas** use flexible, scalable tables. Each row contains a key and related columns stored in a wide format.\n",
    "\n",
    "**Graph schemas**  store data items in collections called nodes. These nodes are connected by edges, which store information about how the nodes are related. However, unlike relational databases, these relationships change as new data is introduced into the nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Database migration**\n",
    "\n",
    "Moving data from one source platform to another target database\n",
    "\n",
    "**Types of databases**\n",
    "\n",
    "- OLTP\n",
    "- OLAP\n",
    "- Row-based: A database that is organized by rows.\n",
    "- Columnar: A database organized by columns instead of rows.\n",
    "- Distributed: A collection of data systems distributed across multiple physical locations.\n",
    "- Single-homed: Databases where all of the data is stored in the same physical location.\n",
    "- Separated storage and compute: Databases where less relevant data is stored remotely, and relevant data is stored locally for analysis.\n",
    "- Combined: Database systems that store and analyze data in the same place.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database comparison checklist\n",
    "\n",
    "<img src=\"../../images/olap_vs_oltp.png\"></img>\n",
    "<img src=\"../../images/row_vs_columnar.png\"></img>\n",
    "<img src=\"../../images/distr_vs_single_homed.png\"></img>\n",
    "<img src=\"../../images/separated_vs_combines.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Database**\n",
    "A collection of data stored in a computer system.\n",
    "\n",
    "**Data warehouse**\n",
    "A specific type of database that consolidates data from multiple source systems for data consistency, accuracy, and efficient access.\n",
    "\n",
    "**Logical data modeling**\n",
    "Representing different tables in the pysical data model\n",
    "\n",
    "\n",
    "### A database schema should include:\n",
    "\n",
    "1. The relevant data\n",
    "2. Names and data types for each column in each table\n",
    "3. Consistent formatting\n",
    "4. Unique keys\n",
    "\n",
    "Developing your schema is an ongoing process.\n",
    "\n",
    "As you receive more data or business needs change, databases and schemas may also need to change.\n",
    "\n",
    "Database optimization is an **iterative process**, which means you may need to check the schema multiple times throughout the database’s useful life."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pipelines and ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data pipeline:**\n",
    "\n",
    "A series of processes that transports data from different sources to their final destination for storage and analysis.\n",
    "\n",
    "Data pipelines are used to define what, where, and how data is combined. They automate the processes involved in extracting, transforming, combining, validating, and loading of data. They also help eliminate errors and latency. \n",
    "\n",
    "**Extract, transform, and load(ETL)**\n",
    "\n",
    "A type of data pipeline that enables data to be gathered from source systems, converted into a useful format, and brought into a data warehouse or other unified destination system.\n",
    "\n",
    "\n",
    "<img src=\"../../images/etl_process.png\"></img>\n",
    "\n",
    "These tables are referred to as **target table:** the predetermined location where pipeline data is sent in order to be acted on.\n",
    "\n",
    "<img src=\"../../images/pipeline_image.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL\n",
    "\n",
    "Stages:\n",
    "\n",
    "- Extract\n",
    "    - Access source systems\n",
    "    - Read and collect the necessary data\n",
    "    - Make the data useful for analysis\n",
    "\n",
    "- Transform\n",
    "    - Consider the structure and format of the destination\n",
    "    - Consider business case requirements\n",
    "    - Validate, clean, and prepare the data for analysis\n",
    "    - Map the data types from the sources to the target systems\n",
    "\n",
    "- Load\n",
    "    - Data is delivered to its target destination\n",
    "    - Data can exist within multiple locations and in multiple formats\n",
    "    \n",
    "\n",
    "Considerations\n",
    "\n",
    "- Key performance indicators\n",
    "- How stakeholders want to view the data\n",
    "- How the data needs to be moved\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business intelligence tools and their applications\n",
    "\n",
    "<img src=\"../../images/tools_1.png\"></img>\n",
    "\n",
    "<img src=\"../../images/tools_2.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL-specific tools and their applications\n",
    "\n",
    "\n",
    "<img src=\"../../images/etl_1.png\"></img>\n",
    "\n",
    "<img src=\"../../images/etl_2.png\"></img>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data-processing with Dataflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Google DataFlow**\n",
    "\n",
    "A serverless data-processing service that reads data from the source, transforms it, and writes it in the destination location\n",
    "\n",
    "\n",
    "**Programming language**\n",
    "\n",
    "A system of words and symbols used to write instructions that computers follow\n",
    "\n",
    "**Python**\n",
    "\n",
    "A general purpose programming language\n",
    "\n",
    "**Python in BI**\n",
    "    - Connect to database system to read and modify files\n",
    "    - Combine with software tools to develop pipelines\n",
    "    - Process big data and perform calculations\n",
    "\n",
    "**Object-oriented programming language**\n",
    "\n",
    "A programming language that is modeled around data objects\n",
    "\n",
    "**Functional programming language**\n",
    "\n",
    "A programming language that is modeled around functions\n",
    "\n",
    "\n",
    "**Interpreted programming language**\n",
    "\n",
    "A programming language that uses an interpreter - usually another program - to read and execute coded instructions.\n",
    "\n",
    "**Compiled programming language**\n",
    "\n",
    "A programming language that compiles coded instructions that are executed directly by the target machine.\n",
    "\n",
    "**Elements of Python**\n",
    "\n",
    "There are a few key elements about Python that are important to understand:\n",
    "\n",
    "- Python is open source and freely available to the public.\n",
    "\n",
    "- It is an interpreted programming language, which means it uses another program to read and execute coded instructions.\n",
    "\n",
    "- Data is stored in data frames, similar to R.\n",
    "\n",
    "- In BI, Python can be used to connect to a database system to work with files.\n",
    "\n",
    "- It is primarily object-oriented.\n",
    "\n",
    "- Formulas, functions, and multiple libraries are readily available.\n",
    "\n",
    "- A community of developers exists for online code support.\n",
    "\n",
    "- Python uses simple syntax for straightforward coding.\n",
    "\n",
    "- It integrates with cloud platforms including Google Cloud, Amazon Web Services, and Azure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organize data in BigQuery\n",
    "\n",
    "BigQuery is a data warehouse on the Google Cloud Platform (GCP) used to query, filter large datasets, aggregate results, and perform complex operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Unify data with target tables**\n",
    "\n",
    "As you have been learning, target tables are predetermined locations where pipeline data is sent in order to be acted on in a database system. Essentially, a source table is where data comes from, and a target table is where it’s going. This reading provides more information about the data-extraction process and how target tables fit into the greater logic of business intelligence processes.\n",
    "\n",
    "**Data extraction**\n",
    "\n",
    "Data extraction is the process of taking data from a source system, such as a database or a SaaS, so that it can be delivered to a destination system for analysis. You might recognize this as the first step in an ETL (extract, transform, and load) pipeline. There are three primary ways that pipelines can extract data from a source in order to deliver it to a target table:\n",
    "\n",
    "- Update notification: The source system issues a notification when a record has been updated, which triggers the extraction.\n",
    "\n",
    "- Incremental extraction: The BI system checks for any data that has changed at the source and ingests these updates.\n",
    "\n",
    "- Full extraction: The BI system extracts a whole table into the target database system.\n",
    "\n",
    "Once data is extracted, it must be loaded into target tables for use. In order to drive intelligent business decisions, users need access to data that is current, clean, and usable. This is why it is important for BI professionals to design target tables that can hold all of the information required to answer business questions.\n",
    "\n",
    "**The importance of target tables**\n",
    "\n",
    "As a BI professional, you will want to take advantage of target tables as a way to unify your data and make it accessible to users. In order to draw insights from a variety of different sources, having a place that contains all of the data from those sources is essential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Working with stakeholders to create a pipeline**\n",
    "\n",
    "Working with stakeholders while designing and iterating on a pipeline system is an important strategy for ensuring that the BI systems you put in place answer their business needs.\n",
    "\n",
    "\n",
    "**The challenge**\n",
    "\n",
    "The Wayfair pricing ecosystem includes thousands of different inputs and outputs across a full catalog of products, which change multiple times a day. All of these inputs and outputs are being generated in different ways from different sources. Because of this, the BI team and other data professionals who needed to access pricing data were having trouble locating, querying, and interpreting the complete dataset. This led to incomplete and often inaccurate insights that weren’t useful for decision makers. \n",
    "\n",
    "To address this, the BI team decided to design and implement a new pipeline system to consolidate all the data stakeholders needed. They also needed to consider a few additional challenges with their pipeline system:\n",
    "\n",
    "Monitoring and reporting around these processes would need to be included in the design to track and manage errors.\n",
    "\n",
    "Data would need to be clean before it could be shared with downstream users. \n",
    "\n",
    "Due to the variety of data types being joined, the BI team also needed to better understand the data relationships so they could accurately consolidate the data. \n",
    "\n",
    "Training sessions would be required to help educate users on how to best access and use the new datasets. \n",
    "\n",
    "These unique challenges meant that it was especially important for the BI team to work closely with stakeholders while developing their new system to address their needs and create something that worked across multiple teams. \n",
    "\n",
    "**The approach** \n",
    "\n",
    "Given the massive amount of data within the system, it was important for the BI team to step back and work with stakeholders to really understand how they were using the data currently. That included understanding the business problems they were trying to solve, the data they were already using and how they were accessing it, and the data they wanted to use but couldn’t access yet. \n",
    "\n",
    "Once they had communicated with stakeholders, the team was able to design a pipeline that achieved three key goals:\n",
    "\n",
    "- All the required data could be made available and easy to understand and use\n",
    "\n",
    "- The system was more efficient and could make data available without delays\n",
    "\n",
    "- The system was designed to scale as the dataset expanded vertically and horizontally to support future growth\n",
    "\n",
    "After this initial design was completed, the system was presented to stakeholders for review to ensure they understood the system and that it met all of their needs. This project required collaboration across a variety of stakeholders and teams:\n",
    "\n",
    "**Software engineers:** The software engineer team were the primary owners and generators of data, so they were key to understanding the current state of the data and helped make it accessible for the BI team to work with.\n",
    "\n",
    "**Data architects:** The BI team consulted with data architects to ensure that the pipeline design was all-encompassing, efficient, and scalable so the BI team could handle the amount of data being ingested by the system and ensure that downstream users would have access to the data as the system was being scaled. \n",
    "\n",
    "**Data professionals:** As the core users, these teams provided the use cases and requirements for the system so that the BI team could ensure that the pipeline addressed their needs. Because each of their respective teams’ needs were different, it was important to ensure the system design and data included was wide enough to account for allof those needs. \n",
    "\n",
    "**Business stakeholders:** As the end users of the insights generated by the entire pipeline, the business stakeholders ensured all development work and use cases were rooted with clear business problems to ensure what the BI team built could be immediately applied to their work. \n",
    "\n",
    "Communicating with all of the stakeholders throughout the design process ensured that the Wayfair BI team created something useful and long-lasting for their organization.\n",
    "\n",
    "**The results**\n",
    "\n",
    "The final pipeline that the BI team implemented achieved a variety of key goals for the entire organization:\n",
    "\n",
    "- It enabled software engineering teams to publish data in real-time for the BI team to use.\n",
    "\n",
    "- It consolidated the different data components into one unified dataset for ease of access and use.\n",
    "\n",
    "- It allowed the BI team to store different data components in their own individual staging layers.\n",
    "\n",
    "- It included additional processes to monitor and report on the system’s performance to inform users where failures were occurring and enable quick fixes.\n",
    "\n",
    "- It created a unified dataset that users could leverage to build metrics and report on data.\n",
    "\n",
    "The greatest benefit of this pipeline solution was that Wayfair now had the ability to provide accurate information in one place for users, eliminating the need to join different sources themselves. This meant that the team could promote more accurate insights for stakeholders and get rid of costly ad-hoc processes. \n",
    "\n",
    "The response cross-team was very positive. The director of analytics at Wayfair said that this was revolutionary for their team’s daily work because they had information on retail price, cost inputs, and product status in the same place for the first time. This was a huge benefit for their processes and to help them handle their data in a more intelligent way. \n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "A significant benefit that business intelligence provides an organization is that it makes the systems and processes more efficient and effective for users across the organization; basically, BI makes everyone’s jobs a little easier. Ensuring that the BI team is tightly aligned with the business stakeholders and other teams is critical to their success. Without great partnership, problems can’t be solved correctly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Glossary terms from week 1**\n",
    "\n",
    "\n",
    "**Attribute:** In a dimensional model, a characteristic or quality used to describe a dimension\n",
    "\n",
    "**Columnar database:** A database organized by columns instead of rows\n",
    "\n",
    "**Combined systems:** Database systems that store and analyze data in the same place\n",
    "\n",
    "**Compiled programming language:** A programming language that compiles coded instructions that are executed directly by the target machine\n",
    "\n",
    "**Data lake:** A database system that stores large amounts of raw data in its original format until it’s needed\n",
    "\n",
    "**Data mart:** A subject-oriented database that can be a subset of a larger data warehouse\n",
    "\n",
    "**Data warehouse:** A specific type of database that consolidates data from multiple source systems for data consistency, accuracy, and efficient access\n",
    "\n",
    "**Database migration:** Moving data from one source platform to another target database. During migration users transition the current database schemas, to a new desired state. This could involve adding tables or columns, splitting fields, removing elements, changing data types or other improvements. The database migration process often requires numerous phases and iterations, as well as lots of testing. These are huge projects for BI teams and you don´t necessarily just want to take the orginal schema and use it in the new one.\n",
    "\n",
    "There are several types of databases:\n",
    "\n",
    "- OLTP\n",
    "- OLAP\n",
    "- Row-based\n",
    "- Columnar\n",
    "- Distributed\n",
    "- Single-homed\n",
    "- Separated storage ad compute\n",
    "- Combined databases\n",
    "\n",
    "**Dimension (data modeling):** A piece of information that provides more detail and context regarding a fact\n",
    "\n",
    "**Dimension table:** The table where the attributes of the dimensions of a fact are stored\n",
    "\n",
    "**Design pattern:** A solution that uses relevant measures and facts to create a model in support of business needs\n",
    "\n",
    "**Dimensional model:** A type of relational model that has been optimized to quickly retrieve data from a data warehouse\n",
    "\n",
    "**Distributed database:** A collection of data systems distributed across multiple physical locations\n",
    "\n",
    "**Fact:** In a dimensional model, a measurement or metric\n",
    "\n",
    "**Fact table:** A table that contains measurements or metrics related to a particular event\n",
    "\n",
    "**Foreign key:** A field within a database table that is a primary key in another table (Refer to primary key)\n",
    "\n",
    "**Functional programming language:** A programming language modeled around functions\n",
    "\n",
    "**Google DataFlow:** A serverless data-processing service that reads data from the source, transforms it, and writes it in the destination location\n",
    "\n",
    "**Interpreted programming language:** A programming language that uses an interpreter, typically another program, to read and execute coded instructions\n",
    "\n",
    "**Logical data modeling:** Representing different tables in the physical data model\n",
    "\n",
    "**Object-oriented programming language:** A programming language modeled around data objects\n",
    "\n",
    "**OLAP (Online Analytical Processing) system:** A tool that has been optimized for analysis in addition to processing and can analyze data from multiple databases\n",
    "\n",
    "**OLTP (Online Transaction Processing) database:** A type of database that has been optimized for data processing instead of analysis\n",
    "\n",
    "**Primary key:** An identifier in a database that references a column or a group of columns in which each row uniquely identifies each record in the table (Refer to foreign key)\n",
    "\n",
    "**Python:** A general purpose programming language\n",
    "\n",
    "**Response time:** The time it takes for a database to complete a user request\n",
    "\n",
    "**Row-based database:** A database that is organized by rows\n",
    "\n",
    "**Separated storage and computing systems:** Databases where data is stored remotely, and relevant data is stored locally for analysis\n",
    "\n",
    "**Single-homed database:** Database where all of the data is stored in the same physical location\n",
    "\n",
    "**Snowflake schema:** An extension of a star schema with additional dimensions and, often, subdimensions\n",
    "\n",
    "**Star schema:** A schema consisting of one fact table that references any number of dimension tables\n",
    "\n",
    "**Target table:** The predetermined location where pipeline data is sent in order to be acted on\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
